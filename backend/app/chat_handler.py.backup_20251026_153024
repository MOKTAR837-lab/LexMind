import os
import json
import hashlib
import requests
from typing import List, Dict, Optional, AsyncGenerator
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer, CrossEncoder
from qdrant_client import QdrantClient
from sqlalchemy import create_engine, text
import redis.asyncio as redis

class ChatHandler:
    def __init__(self, engine: Optional = None):
        self.engine = engine
        self.ollama_url = "http://ollama:11434/api/generate"
        self.model = "mistral"
        
        print("Initialisation ChatHandler OPTIMISE...")
        
        # Redis Cache
        try:
            self.redis = redis.from_url("redis://redis:6379/0", decode_responses=True)
            self.cache_enabled = True
            print("✅ Redis Cache active")
        except Exception as e:
            print(f"⚠️ Redis non disponible: {e}")
            self.cache_enabled = False
        
        # Embedding Model
        try:
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
            print("✅ Embedding model charge")
        except Exception as e:
            print(f"❌ Erreur embedding model: {e}")
            self.embedding_model = None
        
        # Reranker Model
        try:
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            print("✅ Reranker charge")
        except Exception as e:
            print(f"⚠️ Reranker non disponible: {e}")
            self.reranker = None
        
        # Qdrant
        try:
            self.qdrant_client = QdrantClient(host="qdrant", port=6333)
            self.collection_name = "legal_folders"
            info = self.qdrant_client.get_collection(collection_name=self.collection_name)
            print(f"✅ Qdrant OK ({info.points_count} vecteurs)")
            self.use_rag = True
        except Exception as e:
            print(f"❌ Qdrant non disponible: {e}")
            self.use_rag = False
        
        # Ollama
        try:
            response = requests.get("http://ollama:11434/api/tags", timeout=2)
            self.use_ollama = response.status_code == 200
            if self.use_ollama:
                print(f"✅ Ollama OK (modele: {self.model})")
        except:
            self.use_ollama = False
            print("⚠️ Ollama non disponible")
        
        print(f"\n🚀 ChatHandler OPTIMISE pret !")
        print(f"   - RAG: {self.use_rag}")
        print(f"   - Ollama: {self.use_ollama}")
        print(f"   - Cache: {self.cache_enabled}")
        print(f"   - Reranker: {self.reranker is not None}\n")

    async def process_message(self, user_message: str, folders: List[Dict], 
                             filters: Optional[Dict] = None) -> str:
        """Point d'entree principal avec cache"""
        
        # Cache check
        if self.cache_enabled:
            cache_key = self._generate_cache_key(user_message, filters)
            cached = await self._get_from_cache(cache_key)
            if cached:
                print(f"💨 Cache HIT: {cache_key[:16]}...")
                return cached + "\n\n[⚡ Réponse depuis cache]"
        
        # Processing
        if self.use_rag and self.use_ollama:
            response = await self._process_with_rag_optimized(user_message, filters)
        elif self.use_rag:
            response = self._process_rag_only(user_message, filters)
        else:
            response = self._process_basic(user_message, folders)
        
        # Cache store
        if self.cache_enabled and response:
            await self._store_in_cache(cache_key, response, ttl=3600)
        
        return response

    async def process_message_stream(self, user_message: str, folders: List[Dict],
                                    filters: Optional[Dict] = None) -> AsyncGenerator[str, None]:
        """Streaming version"""
        
        # Cache check
        if self.cache_enabled:
            cache_key = self._generate_cache_key(user_message, filters)
            cached = await self._get_from_cache(cache_key)
            if cached:
                yield cached
                yield "\n\n[⚡ Réponse depuis cache]"
                return
        
        # Multi-query decomposition
        sub_queries = self._decompose_query(user_message)
        yield f"[🔍 Analyse en {len(sub_queries)} requêtes...]\n\n"
        
        # Hybrid search
        all_docs = []
        for sq in sub_queries:
            docs = await self._hybrid_search(sq, top_k=5, filters=filters)
            all_docs.extend(docs)
        
        # Deduplication
        seen = set()
        unique_docs = []
        for doc in all_docs:
            if doc['id'] not in seen:
                seen.add(doc['id'])
                unique_docs.append(doc)
        
        # Reranking
        if self.reranker and len(unique_docs) > 3:
            unique_docs = self._rerank_results(user_message, unique_docs)
        
        top_docs = unique_docs[:10]
        
        if not top_docs:
            yield "Aucun dossier pertinent trouvé."
            return
        
        # Context
        context = "\n".join([f"{i+1}. {doc['name']} (score: {doc['score']:.3f})"
                            for i, doc in enumerate(top_docs)])
        
        prompt = f"""Tu es un assistant juridique pour LegalMind.

DOSSIERS PERTINENTS:
{context}

QUESTION: {user_message}

INSTRUCTIONS:
- Réponds de manière professionnelle et précise
- Cite les noms des dossiers pertinents avec [Dossier X]
- Sois concis mais complet (max 500 mots)
- Structure ta réponse avec des paragraphes clairs

RÉPONSE:"""
        
        # Streaming Ollama
        full_response = ""
        try:
            response = requests.post(
                self.ollama_url,
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": True,
                    "options": {"temperature": 0.7, "num_predict": 800}
                },
                stream=True,
                timeout=60
            )
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        chunk = data.get("response", "")
                        if chunk:
                            full_response += chunk
                            yield chunk
                    except json.JSONDecodeError:
                        continue
            
            # Sources
            sources = self._format_sources(top_docs[:5])
            yield f"\n\n{sources}"
            
            # Cache full response
            if self.cache_enabled and full_response:
                await self._store_in_cache(cache_key, full_response + "\n\n" + sources, ttl=3600)
            
        except Exception as e:
            yield f"\n\n⚠️ Erreur: {str(e)}"

    def _decompose_query(self, query: str) -> List[str]:
        """Multi-Query Decomposition"""
        query_lower = query.lower()
        
        # Patterns complexes
        if any(word in query_lower for word in ['et', 'ou', 'ainsi que', 'également']):
            # Decomposition intelligente
            sub_queries = [query]  # Original
            
            # Extraction entites juridiques
            if 'bailleur' in query_lower and 'locataire' in query_lower:
                sub_queries.append(query_lower.replace('et', '').replace('locataire', '').strip())
                sub_queries.append(query_lower.replace('et', '').replace('bailleur', '').strip())
            
            if 'obligation' in query_lower and 'recours' in query_lower:
                sub_queries.append(query_lower.split('et')[0].strip())
                if len(query_lower.split('et')) > 1:
                    sub_queries.append(query_lower.split('et')[1].strip())
            
            return list(set([q for q in sub_queries if len(q) > 10]))
        
        return [query]

    async def _hybrid_search(self, query: str, top_k=10, filters: Optional[Dict] = None) -> List[Dict]:
        """Hybrid Search: Qdrant + PostgreSQL Full-Text"""
        
        results = []
        
        # 1. Vector Search (Qdrant) - 70% weight
        if self.use_rag and self.embedding_model:
            vector_results = self._vector_search(query, top_k=top_k, filters=filters)
            for doc in vector_results:
                doc['search_type'] = 'vector'
                doc['score'] = doc['score'] * 0.7
            results.extend(vector_results)
        
        # 2. Full-Text Search (PostgreSQL) - 30% weight
        if self.engine:
            try:
                with self.engine.connect() as conn:
                    sql = text("""
                        SELECT id, name, 
                               ts_rank(to_tsvector('french', name), plainto_tsquery('french', :query)) as score
                        FROM folders
                        WHERE to_tsvector('french', name) @@ plainto_tsquery('french', :query)
                        ORDER BY score DESC
                        LIMIT :limit
                    """)
                    fulltext_results = conn.execute(sql, {"query": query, "limit": top_k}).fetchall()
                    
                    for row in fulltext_results:
                        results.append({
                            'id': row[0],
                            'name': row[1],
                            'score': float(row[2]) * 0.3,
                            'search_type': 'fulltext'
                        })
            except Exception as e:
                print(f"⚠️ Full-text search error: {e}")
        
        # Merge and sort
        merged = {}
        for doc in results:
            if doc['id'] in merged:
                merged[doc['id']]['score'] += doc['score']
            else:
                merged[doc['id']] = doc
        
        return sorted(merged.values(), key=lambda x: x['score'], reverse=True)[:top_k]

    def _vector_search(self, query: str, top_k=10, filters: Optional[Dict] = None) -> List[Dict]:
        """Vector search with filters"""
        if not self.use_rag or not self.embedding_model:
            return []
        
        try:
            query_vector = self.embedding_model.encode(query)
            
            # Build filter
            qdrant_filter = None
            if filters:
                conditions = []
                if 'date_from' in filters:
                    conditions.append({
                        "key": "created_at",
                        "range": {"gte": filters['date_from']}
                    })
                if 'date_to' in filters:
                    conditions.append({
                        "key": "created_at",
                        "range": {"lte": filters['date_to']}
                    })
                if 'category' in filters:
                    conditions.append({
                        "key": "category",
                        "match": {"value": filters['category']}
                    })
                
                if conditions:
                    qdrant_filter = {"must": conditions}
            
            results = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=query_vector.tolist(),
                limit=top_k,
                score_threshold=0.3,
                query_filter=qdrant_filter
            )
            
            return [{
                'id': hit.id,
                'name': hit.payload.get('name', ''),
                'score': hit.score,
                'metadata': hit.payload
            } for hit in results.points]
            
        except Exception as e:
            print(f"❌ Vector search error: {e}")
            return []

    def _rerank_results(self, query: str, docs: List[Dict]) -> List[Dict]:
        """Rerank with cross-encoder"""
        if not self.reranker or not docs:
            return docs
        
        try:
            pairs = [(query, doc['name']) for doc in docs]
            scores = self.reranker.predict(pairs)
            
            for doc, score in zip(docs, scores):
                doc['rerank_score'] = float(score)
                doc['score'] = (doc['score'] + float(score)) / 2
            
            return sorted(docs, key=lambda x: x['score'], reverse=True)
        except Exception as e:
            print(f"⚠️ Reranking error: {e}")
            return docs

    async def _process_with_rag_optimized(self, user_message: str, filters: Optional[Dict] = None) -> str:
        """Optimized RAG with all features"""
        try:
            # Multi-query
            sub_queries = self._decompose_query(user_message)
            
            # Hybrid search
            all_docs = []
            for sq in sub_queries:
                docs = await self._hybrid_search(sq, top_k=5, filters=filters)
                all_docs.extend(docs)
            
            # Dedup
            seen = set()
            unique_docs = []
            for doc in all_docs:
                if doc['id'] not in seen:
                    seen.add(doc['id'])
                    unique_docs.append(doc)
            
            # Rerank
            if self.reranker and len(unique_docs) > 3:
                unique_docs = self._rerank_results(user_message, unique_docs)
            
            relevant_docs = unique_docs[:10]
            
            if not relevant_docs:
                return "Aucun dossier pertinent trouvé."
            
            context = "\n".join([f"{i+1}. {doc['name']} (score: {doc['score']:.3f})"
                                for i, doc in enumerate(relevant_docs)])
            
            prompt = f"""Tu es un assistant juridique pour LegalMind.

DOSSIERS PERTINENTS:
{context}

QUESTION: {user_message}

INSTRUCTIONS:
- Réponds de manière professionnelle et précise
- Cite les noms des dossiers pertinents
- Sois concis mais complet (max 500 mots)

RÉPONSE:"""
            
            response = requests.post(
                self.ollama_url,
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.7, "num_predict": 800}
                },
                timeout=60
            )
            
            if response.status_code == 200:
                answer = response.json().get("response", "")
                sources = self._format_sources(relevant_docs[:5])
                return f"{answer}\n\n{sources}"
            else:
                return self._process_rag_only(user_message, filters)
                
        except Exception as e:
            return f"⚠️ Erreur: {str(e)}"

    def _process_rag_only(self, user_message: str, filters: Optional[Dict] = None) -> str:
        """RAG without Ollama"""
        relevant_docs = self._vector_search(user_message, top_k=15, filters=filters)
        
        if not relevant_docs:
            return "Aucun dossier correspondant."
        
        categories = {}
        for doc in relevant_docs:
            name = doc['name'].lower()
            if 'plainte' in name:
                categories.setdefault('Plaintes', []).append(doc)
            elif 'argumentaire' in name:
                categories.setdefault('Argumentaires', []).append(doc)
            elif 'vulnérabilité' in name or 'vulnerabilite' in name:
                categories.setdefault('Vulnérabilité', []).append(doc)
            elif 'fraude' in name:
                categories.setdefault('Fraudes', []).append(doc)
            else:
                categories.setdefault('Autres', []).append(doc)
        
        response = f"📊 Analyse de {len(relevant_docs)} dossiers pertinents:\n\n"
        for cat, docs in categories.items():
            response += f"**{cat}** ({len(docs)}):\n"
            for doc in docs[:5]:
                response += f"  • {doc['name']} (score: {doc['score']:.2f})\n"
            response += "\n"
        
        return response.strip()

    def _process_basic(self, user_message: str, folders: List[Dict]) -> str:
        """Fallback sans RAG"""
        msg_lower = user_message.lower()
        
        if any(word in msg_lower for word in ["bonjour", "salut", "hello"]):
            return f"👋 Bonjour ! Assistant LegalMind à votre service.\n{len(folders)} dossiers disponibles."
        
        if "combien" in msg_lower:
            return f"📁 Vous avez {len(folders)} dossiers dans votre système."
        
        return f"💼 LegalMind prêt ! {len(folders)} dossiers disponibles.\nPosez vos questions juridiques."

    def _format_sources(self, docs: List[Dict]) -> str:
        """Format sources"""
        if not docs:
            return ""
        
        sources = "📚 **Sources consultées:**\n"
        for i, doc in enumerate(docs, 1):
            score_emoji = "🟢" if doc['score'] > 0.7 else "🟡" if doc['score'] > 0.5 else "🟠"
            sources += f"  [{i}] {score_emoji} {doc['name']}\n"
        
        return sources

    def _generate_cache_key(self, query: str, filters: Optional[Dict] = None) -> str:
        """Generate cache key"""
        key_data = f"{query}:{json.dumps(filters or {}, sort_keys=True)}"
        return f"chat:{hashlib.md5(key_data.encode()).hexdigest()}"

    async def _get_from_cache(self, key: str) -> Optional[str]:
        """Get from Redis cache"""
        if not self.cache_enabled:
            return None
        try:
            return await self.redis.get(key)
        except:
            return None

    async def _store_in_cache(self, key: str, value: str, ttl: int = 3600):
        """Store in Redis cache"""
        if not self.cache_enabled:
            return
        try:
            await self.redis.setex(key, ttl, value)
        except Exception as e:
            print(f"⚠️ Cache store error: {e}")
